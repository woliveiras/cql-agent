# ğŸ”§ Agente de IA para Reparos Residenciais

Agente de IA especializado em ajudar com pequenos reparos residenciais, construÃ­do com LangChain e Python. Suporta mÃºltiplos provedores de LLM: Ollama (local), OpenAI, Google Gemini e Anthropic.

[![on_push_to_main](https://github.com/woliveiras/cql-agent/actions/workflows/on_push_to_main.yml/badge.svg)](https://github.com/woliveiras/cql-agent/actions/workflows/on_push_to_main.yml)

## âœ¨ Funcionalidades

- ğŸ¤– Chat interativo para perguntas sobre reparos
- ğŸ  Especializado em problemas residenciais
- âš ï¸ Alertas de seguranÃ§a quando necessÃ¡rio
- ğŸ’¡ InstruÃ§Ãµes passo a passo
- ğŸ¯ **MÃºltiplos provedores de LLM suportados:**
  - ğŸ”’ **Ollama** - 100% local e privado (padrÃ£o)
  - ğŸŒ **OpenAI** - GPT-4, GPT-3.5-turbo, etc
  - âœ¨ **Google Gemini** - Gemini 1.5 Flash/Pro
  - ğŸ§  **Anthropic** - Claude 3.5 Sonnet
- ğŸ”„ Sistema de tentativas (atÃ© 3 tentativas antes de sugerir profissional)
- âœ… ValidaÃ§Ã£o de feedback com respostas "sim" ou "nÃ£o"
- ğŸ“ HistÃ³rico de conversaÃ§Ã£o mantido para contexto
- ğŸ¯ DetecÃ§Ã£o automÃ¡tica de sucesso/falha
- ğŸ“š Base de conhecimento a partir de PDFs
- ğŸ” Busca semÃ¢ntica em documentos
- ğŸ’¾ Armazenamento vetorial com ChromaDB
- ğŸ¯ Respostas baseadas em manuais especÃ­ficos
- âš¡ Embeddings com mÃºltiplos provedores
- ğŸŒ Busca web com DuckDuckGo
- ğŸ”„ Fallback automÃ¡tico: RAG â†’ Web â†’ LLM
- ğŸ‡§ğŸ‡· Resultados em portuguÃªs (regiÃ£o br-pt)
- ğŸŒ API REST com FastAPI
- ğŸ“– DocumentaÃ§Ã£o Swagger e ReDoc automÃ¡ticas
- ğŸ”Œ Pipe Function para OpenWebUI
- ğŸ³ Docker Compose para deploy completo
- ğŸ”„ Gerenciamento de sessÃµes
- ğŸ¨ Interface web moderna
- ğŸ”’ Privacidade mantida (DuckDuckGo nÃ£o rastreia)
- ğŸ›¡ï¸ SeguranÃ§a reforÃ§ada com sanitizaÃ§Ã£o e guardrails
- âœ… ValidaÃ§Ã£o rigorosa de entrada (Pydantic nativo)
- ğŸš« ProteÃ§Ã£o contra injection (SQL, XSS, Command)
- ğŸ¯ Guardrails de conteÃºdo (apenas reparos residenciais)
- âš¡ Performance otimizada (async/await)

## ğŸ—ï¸ Arquitetura

O projeto Ã© dividido em trÃªs componentes principais:

1. **Backend (Python + FastAPI)**: API REST com agente de IA, RAG e ferramentas
2. **Frontend (React + TypeScript)**: Interface web moderna para interaÃ§Ã£o
3. **Ollama (Docker)**: Servidor LLM local para processamento

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚â”€â”€â”€â”€â”€â–¶â”‚   Backend   â”‚â”€â”€â”€â”€â”€â–¶â”‚   Ollama    â”‚
â”‚  React/Vite â”‚ HTTP â”‚ FastAPI/LLM â”‚ HTTP â”‚  LLM Local  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”œâ”€â”€â–¶ ChromaDB (RAG)
                            â””â”€â”€â–¶ DuckDuckGo (Web Search)
```

## ğŸš€ Como usar

### 1. PrÃ©-requisitos

**Backend:**

- Python 3.12+
- UV (gerenciador de pacotes Python)
- Docker e Docker Compose

**Frontend (Opcional):**

- Node.js 18+ ou 20+
- pnpm 10+

### 2. Configurar Provedor de LLM

Copie o arquivo de exemplo e configure seu provedor:

```bash
cp .env.example .env
```

Edite o arquivo `.env` e escolha seu provedor:

#### OpÃ§Ã£o A: Ollama (Local - Gratuito) â­ PadrÃ£o

```bash
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

Depois, inicie o Ollama:

Se estiver usando o Ollama diretamente em seu S.O., lembre-se de executar o app.

Em seguida, baixe os modelos atravÃ©s do terminal:

```bash
# Baixar os modelos (primeira vez)
ollama pull qwen2.5:3b
ollama pull nomic-embed-text
```

Se estiver usando via Docker, execute:

```bash
# Subir o container do Ollama
docker-compose up -d

# Baixar os modelos (primeira vez)
docker exec -it ollama ollama pull qwen2.5:3b
docker exec -it ollama ollama pull nomic-embed-text
```

#### OpÃ§Ã£o B: OpenAI

```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...  # Sua chave da API OpenAI
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

Obtenha sua chave em: <https://platform.openai.com/api-keys>

**Para instalar o suporte OpenAI:**

```bash
uv sync --extra openai
```

#### OpÃ§Ã£o C: Google Gemini

```bash
LLM_PROVIDER=gemini
GEMINI_API_KEY=...  # Sua chave da API Gemini
GEMINI_MODEL=gemini-1.5-flash
GEMINI_EMBEDDING_MODEL=models/embedding-001
```

Obtenha sua chave em: <https://makersuite.google.com/app/apikey>

**Para instalar o suporte Gemini:**

```bash
uv sync --extra google
```

#### OpÃ§Ã£o D: Anthropic (Claude)

```bash
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...  # Sua chave da API Anthropic
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Para embeddings, use um dos outros provedores:
EMBEDDING_PROVIDER=openai  # ou ollama
```

Obtenha sua chave em: <https://console.anthropic.com/settings/keys>

**Para instalar o suporte Anthropic:**

```bash
uv sync --extra anthropic
```

#### Instalar Todos os Provedores

```bash
uv sync --extra all-providers
```

### 3. Configurar RAG

```bash
# 1. Adicionar PDFs na pasta pdfs/
# Coloque manuais sobre reparos residenciais

# 2. Processar PDFs e criar base de conhecimento
uv run scripts/setup_rag.py

# Isso irÃ¡ criar o banco vetorial em chroma_db/
```

### 3. Instalar dependÃªncias

```bash
uv sync
```

### 4. Executar o agente

#### OpÃ§Ã£o 1: CLI (Linha de Comando)

```bash
# Ativar o ambiente virtual (opcional, uv faz isso automaticamente)
source .venv/bin/activate

# Executar o agente
uv run agent.py
```

#### OpÃ§Ã£o 2: API REST (FastAPI)

```bash
# Desenvolvimento (com auto-reload)
uv run uvicorn api.app:app --host 0.0.0.0 --port 5000 --reload

# ProduÃ§Ã£o (com mÃºltiplos workers)
uv run uvicorn api.app:app --host 0.0.0.0 --port 5000 --workers 4

# Acessar documentaÃ§Ã£o Swagger
# http://localhost:5000/docs

# Acessar documentaÃ§Ã£o ReDoc
# http://localhost:5000/redoc

# Testar API (script automatizado)
./test_api.sh

# Ou testar manualmente
uv run python test_api.py
```

> ğŸ“š **Guia completo:** Veja [GUIA_DEPLOY.md](docs/GUIA_DEPLOY.md) para mais opÃ§Ãµes de execuÃ§Ã£o

#### OpÃ§Ã£o 3: Frontend Web (React)

```bash
# Navegar para a pasta do frontend
cd web

# Instalar dependÃªncias (primeira vez)
pnpm install

# Iniciar servidor de desenvolvimento
pnpm dev

# Acessar interface web
# http://localhost:5173
```

> ğŸ’¡ **Nota:** O frontend requer que a API REST esteja rodando na porta 5000

#### OpÃ§Ã£o 4: Docker Compose (Deploy Completo)

```bash
# Iniciar todos os serviÃ§os (Ollama + API + OpenWebUI)
docker-compose up -d

# Acessar OpenWebUI
# http://localhost:8080
```

Em ambiente local, vocÃª pode usar a Pipe Function integrada ao OpenWebUI e desabilitar a autenticaÃ§Ã£o para facilitar os testes.

Para desabilitar autenticaÃ§Ã£o, edite o arquivo `docker-compose.yml` e defina:

```yaml
  - WEBUI_AUTH=false
```

Os detalhes de uso de Pipe Function estÃ£o na documentaÃ§Ã£o: [IntegraÃ§Ã£o com OpenWebUI](docs/INTEGRACAO_OPENWEBUI.md)

## ğŸ’¬ Exemplo de uso

Exemplo de interaÃ§Ã£o com o agente via CLI:

```bash
============================================================
ğŸ”§ CQL AI Agent - Assistente de Reparos Residenciais
============================================================

Inicializando o agente...


âœ… Agente inicializado com sucesso!

ğŸ’¡ Dica: O agente tentarÃ¡ ajudÃ¡-lo atÃ© 3 vezes antes de sugerir um profissional

ğŸ“ Comandos: 'sair' para encerrar | 'novo' para um novo problema

ğŸ‘¤ VocÃª: Como posso consertar uma torneira que estÃ¡ pingando?

ğŸ¤– Agente: Para consertar uma torneira pingando, siga estes passos:

1. **Feche o registro de Ã¡gua** da torneira
2. Abra a torneira para liberar pressÃ£o restante
3. Remova o cabo da torneira
4. Desatarraxe a peÃ§a de vedaÃ§Ã£o
5. Substitua o anel de borracha (O-ring) ou a vÃ¡lvula
6. Remonte tudo na ordem inversa
7. Abra o registro novamente

âš ï¸ **SeguranÃ§a**: Se nÃ£o se sentir confortÃ¡vel, chame um encanador!

O problema foi resolvido? Responda com 'sim' ou 'nÃ£o'.

ğŸ‘¤ VocÃª: 
```

## ğŸ› ï¸ Tecnologias Utilizadas

**Backend:**

- **Python** - Linguagem de programaÃ§Ã£o
- **UV** - Gerenciador de pacotes e ambientes virtuais
- **LangChain** - Framework para construÃ§Ã£o de aplicaÃ§Ãµes com LLMs
- **MÃºltiplos provedores LLM:**
  - **LangChain-Ollama** - Modelos locais com Ollama
  - **LangChain-OpenAI** - GPT-4, GPT-3.5-turbo (opcional)
  - **LangChain-Google-GenAI** - Gemini 1.5 Flash/Pro (opcional)
  - **LangChain-Anthropic** - Claude 3.5 Sonnet (opcional)
- **Pydantic** - ValidaÃ§Ã£o de dados
- **Docker** - ContainerizaÃ§Ã£o
- **FastAPI** - Framework web moderno para API REST
- **Uvicorn** - Servidor ASGI de alta performance
- **ChromaDB** - Banco de dados vetorial para RAG
- **DuckDuckGo** - Busca web gratuita e privada

**Frontend:**

- **React** - Biblioteca para interfaces de usuÃ¡rio
- **TypeScript** - JavaScript tipado
- **Vite** - Build tool moderna
- **EmotionCSS** - Styling CSS-in-JS
- **Zustand** - State management
- **React Query** - Data fetching e cache
- **React Router** - Roteamento
- **Vitest** - Framework de testes
- **Biome.js** - Linting e formataÃ§Ã£o

## ğŸ’¡ Boas PrÃ¡ticas do CÃ³digo

Este projeto segue boas prÃ¡ticas de desenvolvimento:

- âœ… **CÃ³digo limpo**: ComentÃ¡rios apenas onde agregam valor real
- âœ… **Type hints**: Tipagem estÃ¡tica com Pydantic
- âœ… **DocumentaÃ§Ã£o**: Docstrings significativas em funÃ§Ãµes principais
- âœ… **ModularizaÃ§Ã£o**: CÃ³digo organizado em mÃ³dulos (prompts, rag, tools, api, llm)
- âœ… **Factory Pattern**: AbstraÃ§Ã£o de provedores LLM atravÃ©s de factories
- âœ… **ValidaÃ§Ã£o**: ValidaÃ§Ã£o de entrada/saÃ­da com Pydantic
- âœ… **Logging**: Sistema de logs estruturado
- âœ… **Testes automatizados**: SuÃ­te completa com 136+ testes (unit + integration)
- âœ… **Coverage**: Cobertura de cÃ³digo com pytest-cov
- âœ… **CI/CD**: GitHub Actions com testes automÃ¡ticos
- âœ… **ContainerizaÃ§Ã£o**: Deploy completo com Docker Compose

## ğŸ”Œ Provedores de LLM Suportados

O projeto suporta mÃºltiplos provedores atravÃ©s de uma camada de abstraÃ§Ã£o:

### Arquitetura de Provedores

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           RepairAgent / VectorStore          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   LLMFactory /     â”‚
         â”‚ EmbeddingsFactory  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama  â”‚  â”‚ OpenAI  â”‚  â”‚ Gemini  â”‚  â”‚Anthropicâ”‚
â”‚ (Local) â”‚  â”‚   API   â”‚  â”‚   API   â”‚  â”‚   API   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ComparaÃ§Ã£o de Provedores

| Provedor | Custo | Privacidade | Velocidade | Qualidade | Embeddings |
|----------|-------|-------------|------------|-----------|------------|
| **Ollama** | ğŸŸ¢ Gratuito | ğŸŸ¢ 100% Local | ğŸŸ¡ MÃ©dia | ğŸŸ¢ Boa | âœ… Sim |
| **OpenAI** | ğŸ”´ Pago | ğŸ”´ Cloud | ğŸŸ¢ RÃ¡pida | ğŸŸ¢ Excelente | âœ… Sim |
| **Gemini** | ğŸŸ¡ Free Tier | ğŸ”´ Cloud | ğŸŸ¢ RÃ¡pida | ğŸŸ¢ Excelente | âœ… Sim |
| **Anthropic** | ğŸ”´ Pago | ğŸ”´ Cloud | ğŸŸ¢ RÃ¡pida | ğŸŸ¢ Excelente | âŒ NÃ£o* |

*\* Anthropic nÃ£o possui embeddings prÃ³prios. Use outro provedor para embeddings.*

### ConfiguraÃ§Ã£o via VariÃ¡veis de Ambiente

Toda a configuraÃ§Ã£o Ã© feita atravÃ©s do arquivo `.env`:

```bash
# Escolher provedor principal
LLM_PROVIDER=ollama  # ou openai, gemini, anthropic

# Embeddings podem usar provedor diferente
EMBEDDING_PROVIDER=ollama  # ou openai, gemini

# ConfiguraÃ§Ãµes especÃ­ficas do provedor escolhido
OLLAMA_MODEL=qwen2.5:3b
OPENAI_MODEL=gpt-4o-mini
GEMINI_MODEL=gemini-1.5-flash
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

### Custos Estimados (APIs Pagas)

**OpenAI (Novembro 2024):**

- GPT-4o-mini: ~$0.15/$0.60 por 1M tokens (input/output)
- text-embedding-3-small: ~$0.02 por 1M tokens

**Google Gemini:**

- Gemini 1.5 Flash: Gratuito atÃ© 15 req/min
- Acima: ~$0.35/$1.05 por 1M tokens

**Anthropic:**

- Claude 3.5 Sonnet: ~$3/$15 por 1M tokens

ğŸ’¡ **Dica:** Para uso pessoal/experimental, Ollama (gratuito) ou Gemini Free Tier sÃ£o Ã³timas opÃ§Ãµes!

## ğŸ“ Estrutura do Projeto

```text
cql-agent/
â”œâ”€â”€ agents/                    # ğŸ¤– Agentes de IA
â”‚   â”œâ”€â”€ llm/                   # ğŸ”Œ Gerenciamento de provedores LLM
â”‚   â”‚   â”œâ”€â”€ __init__.py        # Enums e configuraÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ factory.py         # Factory para criar LLMs
â”‚   â”‚   â””â”€â”€ embeddings_factory.py  # Factory para embeddings
â”‚   â”œâ”€â”€ repair_agent/          # Agente principal de reparos
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent.py           # CÃ³digo principal do agente
â”‚   â”‚   â””â”€â”€ prompts/           # MÃ³dulo de prompts organizados
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ base.py        # Prompts base do sistema
â”‚   â”‚       â”œâ”€â”€ states.py      # Estados da conversaÃ§Ã£o
â”‚   â”‚       â”œâ”€â”€ messages.py    # Templates de mensagens
â”‚   â”‚       â””â”€â”€ README.md
â”‚   â”œâ”€â”€ rag/                   # ğŸ“š MÃ³dulo RAG (Retrieval-Augmented Generation)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py          # Carrega e processa PDFs
â”‚   â”‚   â”œâ”€â”€ vectorstore.py     # Gerencia ChromaDB
â”‚   â”‚   â”œâ”€â”€ retriever.py       # Busca documentos semÃ¢nticos
â”‚   â”‚   â””â”€â”€ pdfs/              # PDFs de exemplo
â”‚   â””â”€â”€ tools/                 # ğŸ”§ Ferramentas do agente
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ web_search.py      # Busca web (DuckDuckGo)
â”œâ”€â”€ api/                       # ğŸŒ API REST
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                 # FastAPI + Swagger automÃ¡tico
â”‚   â”œâ”€â”€ gunicorn.conf.py       # ConfiguraÃ§Ã£o Gunicorn (produÃ§Ã£o)
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ security/              # ğŸ›¡ï¸ MÃ³dulo de seguranÃ§a
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sanitizer.py       # SanitizaÃ§Ã£o de entrada
â”‚   â”‚   â”œâ”€â”€ guardrails.py      # ValidaÃ§Ã£o de conteÃºdo com SpaCy
â”‚   â”‚   â”œâ”€â”€ ner_repair.py      # Named Entity Recognition
â”‚   â”‚   â”œâ”€â”€ context_analyzer.py  # AnÃ¡lise de contexto sintÃ¡tico
â”‚   â”‚   â”œâ”€â”€ intention_analyzer.py  # AnÃ¡lise de intenÃ§Ã£o comunicativa
â”‚   â”‚   â””â”€â”€ README.md          # DocumentaÃ§Ã£o de seguranÃ§a
â”‚   â””â”€â”€ tests/                 # ğŸ§ª Testes automatizados
â”‚       â”œâ”€â”€ conftest.py        # Fixtures compartilhadas
â”‚       â”œâ”€â”€ unit/              # Testes unitÃ¡rios
â”‚       â”‚   â””â”€â”€ security/      # Testes de seguranÃ§a
â”‚       â”œâ”€â”€ integration/       # Testes de integraÃ§Ã£o
â”‚       â””â”€â”€ README.md          # DocumentaÃ§Ã£o de testes
â”œâ”€â”€ web/                       # ğŸ¨ Frontend React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # Componentes reutilizÃ¡veis
â”‚   â”‚   â”œâ”€â”€ containers/        # Componentes agregadores
â”‚   â”‚   â”œâ”€â”€ pages/             # PÃ¡ginas da aplicaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ hooks/             # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/          # API clients
â”‚   â”‚   â”œâ”€â”€ store/             # Zustand stores
â”‚   â”‚   â”œâ”€â”€ styles/            # Theme e estilos globais
â”‚   â”‚   â””â”€â”€ test/              # Setup de testes
â”‚   â”œâ”€â”€ public/                # Arquivos estÃ¡ticos
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ biome.json             # ConfiguraÃ§Ã£o Biome.js
â”‚   â””â”€â”€ README.md              # DocumentaÃ§Ã£o do frontend
â”œâ”€â”€ openwebui/                 # ğŸ”Œ IntegraÃ§Ã£o OpenWebUI
â”‚   â””â”€â”€ pipe.py                # Pipe Function
â”œâ”€â”€ scripts/                   # ğŸ“œ Scripts auxiliares
â”‚   â””â”€â”€ setup_rag.py           # Processa PDFs e cria base
â”œâ”€â”€ docs/                      # ğŸ“– DocumentaÃ§Ã£o detalhada
â”‚   â”œâ”€â”€ GUIA_DEPLOY.md         # Deploy e execuÃ§Ã£o (dev + prod)
â”‚   â”œâ”€â”€ GUIA_SWAGGER.md        # DocumentaÃ§Ã£o Swagger
â”‚   â”œâ”€â”€ INTEGRACAO_OPENWEBUI.md  # IntegraÃ§Ã£o com OpenWebUI
â”‚   â””â”€â”€ QUICK_START_RAG.md     # Guia rÃ¡pido RAG
â”œâ”€â”€ pdfs/                      # ğŸ“„ PDFs de conhecimento (adicionar aqui)
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ chroma_db/                 # ğŸ’¾ Base vetorial (gerado automaticamente)
â”‚   â””â”€â”€ chroma.sqlite3
â”œâ”€â”€ docker-compose.yml         # ğŸ³ Deploy completo (Ollama + API)
â”œâ”€â”€ Dockerfile                 # ğŸ³ Docker para API
â”œâ”€â”€ setup.sh                   # ğŸš€ Script de setup inicial
â”œâ”€â”€ test_api.sh                # ğŸ§ª Script de testes da API
â”œâ”€â”€ test_security.sh           # ğŸ›¡ï¸ Script de testes de seguranÃ§a
â”œâ”€â”€ pyproject.toml             # ğŸ“¦ DependÃªncias Python
â””â”€â”€ README.md                  # ğŸ“˜ Este arquivo
```

## ğŸ›¡ï¸ SeguranÃ§a

O projeto implementa mÃºltiplas camadas de seguranÃ§a:

### 1. ValidaÃ§Ã£o de Schema (Pydantic)

- Mensagens: 1-4096 caracteres
- Session ID: alfanumÃ©rico com _ e -
- ValidaÃ§Ã£o rigorosa de tipos

### 2. SanitizaÃ§Ã£o de Entrada

- Remove caracteres nulos (`\x00`)
- Detecta SQL injection
- Detecta XSS (Cross-Site Scripting)
- Detecta command injection
- Previne DoS por repetiÃ§Ã£o

### 3. Guardrails de ConteÃºdo

- Valida se mensagem Ã© sobre reparos residenciais
- Bloqueia conteÃºdo proibido (ilegal, adulto, jailbreak)
- Score de relevÃ¢ncia (0.0 a 1.0)
- ValidaÃ§Ã£o adicional com LLM

### 4. Tratamento de Erros

- Retorna 400 Bad Request para entrada invÃ¡lida
- NÃ£o vaza detalhes internos
- Logs detalhados para auditoria

### Testes Automatizados

O projeto possui uma suÃ­te completa de testes organizados por tipo:

```bash
# Executar todos os testes
pytest

# Testes unitÃ¡rios (136 testes)
pytest api/tests/unit/ -v

# Testes de integraÃ§Ã£o (API deve estar rodando)
pytest api/tests/integration/ -v

# Testes com cobertura
pytest --cov=api --cov=agents --cov-report=html

# Ver relatÃ³rio de cobertura
open htmlcov/index.html
```

**Estrutura de testes:**
- **Unit**: Testes de componentes isolados (sanitizaÃ§Ã£o, NER, anÃ¡lise de contexto, etc.)
- **Integration**: Testes de fluxos completos da API e seguranÃ§a

Veja [api/tests/README.md](api/tests/README.md) para documentaÃ§Ã£o completa.

## ğŸ› Troubleshooting

### Erro de conexÃ£o com Ollama

```bash
âŒ Erro: Connection refused
```

**SoluÃ§Ã£o**: Certifique-se que o Ollama estÃ¡ rodando:

```bash
docker-compose ps
docker-compose up -d
```

### Modelo nÃ£o encontrado

```bash
âŒ Erro: model 'qwen2.5:3b' not found
```

**SoluÃ§Ã£o**: Baixe o modelo:

```bash
docker exec -it ollama ollama pull qwen2.5:3b
```

## ğŸ“ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto e estÃ¡ disponÃ­vel sob a licenÃ§a MIT.
