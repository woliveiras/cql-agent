# ğŸ”§ Agente de IA para Reparos Residenciais

Agente de IA especializado em ajudar com pequenos reparos residenciais, construÃ­do com LangChain, Ollama e Python.

## ğŸ“‹ Fase 1 - Agente BÃ¡sico

Nesta primeira fase, implementamos um agente bÃ¡sico que responde perguntas sobre reparos residenciais usando um modelo LLM local via Ollama.

### âœ¨ Funcionalidades

- ğŸ¤– Chat interativo para perguntas sobre reparos
- ğŸ  Especializado em problemas residenciais
- âš ï¸ Alertas de seguranÃ§a quando necessÃ¡rio
- ğŸ’¡ InstruÃ§Ãµes passo a passo
- ğŸ”’ 100% local e privado (usando Ollama)
- ğŸ”„ Sistema de tentativas (atÃ© 3 tentativas antes de sugerir profissional)
- âœ… ValidaÃ§Ã£o de feedback com respostas "sim" ou "nÃ£o"
- ğŸ“ HistÃ³rico de conversaÃ§Ã£o mantido para contexto
- ğŸ¯ DetecÃ§Ã£o automÃ¡tica de sucesso/falha

## ğŸš€ Como usar

### 1. PrÃ©-requisitos

- Python 3.10+
- Docker e Docker Compose
- UV (gerenciador de pacotes Python)

### 2. Iniciar o Ollama

```bash
# Subir o container do Ollama
docker-compose up -d

# Baixar o modelo (primeira vez)
docker exec -it ollama ollama pull qwen2.5:3b
```

### 3. Instalar dependÃªncias

```bash
uv sync
```

### 4. Executar o agente

```bash
# Ativar o ambiente virtual (opcional, uv faz isso automaticamente)
source .venv/bin/activate

# Executar o agente
uv run agent.py
```

## ğŸ’¬ Exemplo de uso

```bash
============================================================
ğŸ”§ CQL AI Agent - Assistente de Reparos Residenciais
============================================================

Inicializando o agente...


âœ… Agente inicializado com sucesso!

ğŸ’¡ Dica: O agente tentarÃ¡ ajudÃ¡-lo atÃ© 3 vezes antes de sugerir um profissional

ğŸ“ Comandos: 'sair' para encerrar | 'novo' para um novo problema

ğŸ‘¤ VocÃª: Como posso consertar uma torneira que estÃ¡ pingando?

ğŸ¤– Agente: Para consertar uma torneira pingando, siga estes passos:

1. **Feche o registro de Ã¡gua** da torneira
2. Abra a torneira para liberar pressÃ£o restante
3. Remova o cabo da torneira
4. Desatarraxe a peÃ§a de vedaÃ§Ã£o
5. Substitua o anel de borracha (O-ring) ou a vÃ¡lvula
6. Remonte tudo na ordem inversa
7. Abra o registro novamente

âš ï¸ **SeguranÃ§a**: Se nÃ£o se sentir confortÃ¡vel, chame um encanador!

O problema foi resolvido? Responda com 'sim' ou 'nÃ£o'.

ğŸ‘¤ VocÃª: 
```

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python** - Linguagem de programaÃ§Ã£o
- **UV** - Gerenciador de pacotes e ambientes virtuais
- **LangChain** - Framework para construÃ§Ã£o de aplicaÃ§Ãµes com LLMs
- **LangChain-Ollama** - IntegraÃ§Ã£o do LangChain com Ollama
- **Pydantic** - ValidaÃ§Ã£o de dados
- **Ollama** - ExecuÃ§Ã£o local de modelos LLM
- **Docker** - ContainerizaÃ§Ã£o do Ollama

## ğŸ“ Estrutura do Projeto

```text
cql-agent/
â”œâ”€â”€ agent.py              # CÃ³digo principal do agente
â”œâ”€â”€ prompts/              # MÃ³dulo de prompts organizados
â”‚   â”œâ”€â”€ __init__.py       # Exporta todos os prompts
â”‚   â”œâ”€â”€ base.py           # Prompt base do sistema
â”‚   â”œâ”€â”€ states.py         # Prompts por estado
â”‚   â”œâ”€â”€ messages.py       # Mensagens de resposta
â”‚   â””â”€â”€ README.md         # DocumentaÃ§Ã£o dos prompts
â”œâ”€â”€ docker-compose.yml    # ConfiguraÃ§Ã£o do Ollama
â”œâ”€â”€ pyproject.toml        # DependÃªncias do projeto
â”œâ”€â”€ uv.lock              # Lock file das dependÃªncias
â””â”€â”€ README.md            # Este arquivo
```

## ğŸ”„ PrÃ³ximas Fases

- **Fase 2**: RAG com base de conhecimento (PDFs)
- **Fase 3**: IntegraÃ§Ã£o com DuckDuckGo para busca na internet
- **Fase 4**: API Flask e integraÃ§Ã£o com OpenWebUI

## ğŸ› Troubleshooting

### Erro de conexÃ£o com Ollama

```bash
âŒ Erro: Connection refused
```

**SoluÃ§Ã£o**: Certifique-se que o Ollama estÃ¡ rodando:

```bash
docker-compose ps
docker-compose up -d
```

### Modelo nÃ£o encontrado

```bash
âŒ Erro: model 'qwen2.5:3b' not found
```

**SoluÃ§Ã£o**: Baixe o modelo:

```bash
docker exec -it ollama ollama pull qwen2.5:3b
```

## ğŸ“ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto e estÃ¡ disponÃ­vel sob a licenÃ§a MIT.
