# ğŸ”§ Agente de IA para Reparos Residenciais

Agente de IA especializado em ajudar com pequenos reparos residenciais, construÃ­do com LangChain, Ollama e Python.

## ğŸ“‹ Fases do Projeto

### Fase 1 - Agente BÃ¡sico

Agente bÃ¡sico que responde perguntas usando modelo LLM local.

### Fase 2 - RAG

Sistema de recuperaÃ§Ã£o de informaÃ§Ãµes de PDFs com ChromaDB.

### âœ¨ Funcionalidades

#### Fase 1 (Agente BÃ¡sico)

- ğŸ¤– Chat interativo para perguntas sobre reparos
- ğŸ  Especializado em problemas residenciais
- âš ï¸ Alertas de seguranÃ§a quando necessÃ¡rio
- ğŸ’¡ InstruÃ§Ãµes passo a passo
- ğŸ”’ 100% local e privado (usando Ollama)
- ğŸ”„ Sistema de tentativas (atÃ© 3 tentativas antes de sugerir profissional)
- âœ… ValidaÃ§Ã£o de feedback com respostas "sim" ou "nÃ£o"
- ğŸ“ HistÃ³rico de conversaÃ§Ã£o mantido para contexto
- ğŸ¯ DetecÃ§Ã£o automÃ¡tica de sucesso/falha

#### Fase 2 (RAG)

- ğŸ“š Base de conhecimento a partir de PDFs
- ğŸ” Busca semÃ¢ntica em documentos
- ğŸ’¾ Armazenamento vetorial com ChromaDB
- ğŸ¯ Respostas baseadas em manuais especÃ­ficos
- âš¡ Embeddings locais com Ollama

## ğŸš€ Como usar

### 1. PrÃ©-requisitos

- Python 3.10+
- Docker e Docker Compose
- UV (gerenciador de pacotes Python)

### 2. Iniciar o Ollama

```bash
# Subir o container do Ollama
docker-compose up -d

# Baixar os modelos (primeira vez)
docker exec -it ollama ollama pull qwen2.5:3b
docker exec -it ollama ollama pull nomic-embed-text
```

### 2.5. Configurar RAG (Opcional - Fase 2)

```bash
# 1. Adicionar PDFs na pasta pdfs/
# Coloque manuais sobre reparos residenciais

# 2. Processar PDFs e criar base de conhecimento
uv run scripts/setup_rag.py

# Isso irÃ¡ criar o banco vetorial em chroma_db/
```

### 3. Instalar dependÃªncias

```bash
uv sync
```

### 4. Executar o agente

```bash
# Ativar o ambiente virtual (opcional, uv faz isso automaticamente)
source .venv/bin/activate

# Executar o agente
uv run agent.py
```

## ğŸ’¬ Exemplo de uso

```bash
============================================================
ğŸ”§ CQL AI Agent - Assistente de Reparos Residenciais
============================================================

Inicializando o agente...


âœ… Agente inicializado com sucesso!

ğŸ’¡ Dica: O agente tentarÃ¡ ajudÃ¡-lo atÃ© 3 vezes antes de sugerir um profissional

ğŸ“ Comandos: 'sair' para encerrar | 'novo' para um novo problema

ğŸ‘¤ VocÃª: Como posso consertar uma torneira que estÃ¡ pingando?

ğŸ¤– Agente: Para consertar uma torneira pingando, siga estes passos:

1. **Feche o registro de Ã¡gua** da torneira
2. Abra a torneira para liberar pressÃ£o restante
3. Remova o cabo da torneira
4. Desatarraxe a peÃ§a de vedaÃ§Ã£o
5. Substitua o anel de borracha (O-ring) ou a vÃ¡lvula
6. Remonte tudo na ordem inversa
7. Abra o registro novamente

âš ï¸ **SeguranÃ§a**: Se nÃ£o se sentir confortÃ¡vel, chame um encanador!

O problema foi resolvido? Responda com 'sim' ou 'nÃ£o'.

ğŸ‘¤ VocÃª: 
```

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python** - Linguagem de programaÃ§Ã£o
- **UV** - Gerenciador de pacotes e ambientes virtuais
- **LangChain** - Framework para construÃ§Ã£o de aplicaÃ§Ãµes com LLMs
- **LangChain-Ollama** - IntegraÃ§Ã£o do LangChain com Ollama
- **Pydantic** - ValidaÃ§Ã£o de dados
- **Ollama** - ExecuÃ§Ã£o local de modelos LLM
- **Docker** - ContainerizaÃ§Ã£o do Ollama

## ğŸ“ Estrutura do Projeto

```text
cql-agent/
â”œâ”€â”€ agent.py              # CÃ³digo principal do agente
â”œâ”€â”€ prompts/              # MÃ³dulo de prompts organizados
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ states.py
â”‚   â”œâ”€â”€ messages.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ rag/                  # MÃ³dulo RAG
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py         # Carrega e processa PDFs
â”‚   â”œâ”€â”€ vectorstore.py    # Gerencia ChromaDB
â”‚   â””â”€â”€ retriever.py      # Busca documentos
â”œâ”€â”€ scripts/              # Scripts auxiliares
â”‚   â””â”€â”€ setup_rag.py      # Processa PDFs e cria base
â”œâ”€â”€ pdfs/                 # PDFs de conhecimento (adicionar aqui)
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ chroma_db/            # Base vetorial (gerado automaticamente)
â”œâ”€â”€ docker-compose.yml    # ConfiguraÃ§Ã£o do Ollama
â”œâ”€â”€ pyproject.toml        # DependÃªncias
â””â”€â”€ README.md             # Este arquivo
```

## ğŸ”„ PrÃ³ximas Fases

- âœ… **Fase 1**: Agente bÃ¡sico (ConcluÃ­da)
- ğŸš€ **Fase 2**: RAG com base de conhecimento (Em progresso)
- **Fase 3**: IntegraÃ§Ã£o com DuckDuckGo para busca na internet
- **Fase 4**: API Flask e integraÃ§Ã£o com OpenWebUI

## ğŸ› Troubleshooting

### Erro de conexÃ£o com Ollama

```bash
âŒ Erro: Connection refused
```

**SoluÃ§Ã£o**: Certifique-se que o Ollama estÃ¡ rodando:

```bash
docker-compose ps
docker-compose up -d
```

### Modelo nÃ£o encontrado

```bash
âŒ Erro: model 'qwen2.5:3b' not found
```

**SoluÃ§Ã£o**: Baixe o modelo:

```bash
docker exec -it ollama ollama pull qwen2.5:3b
```

## ğŸ“ LicenÃ§a

Este projeto Ã© de cÃ³digo aberto e estÃ¡ disponÃ­vel sob a licenÃ§a MIT.
