# üîå Guia de Provedores LLM

Este documento explica como configurar e usar diferentes provedores de LLM (Large Language Models) no CQL Agent.

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Arquitetura](#arquitetura)
- [Provedores Suportados](#provedores-suportados)
  - [Ollama (Local)](#ollama-local)
  - [OpenAI](#openai)
  - [Google Gemini](#google-gemini)
  - [Anthropic (Claude)](#anthropic-claude)
- [Configura√ß√£o](#configura√ß√£o)
- [Migra√ß√£o entre Provedores](#migra√ß√£o-entre-provedores)
- [Troubleshooting](#troubleshooting)

## üéØ Vis√£o Geral

O CQL Agent foi projetado para ser **agn√≥stico de provedor**, permitindo que voc√™ escolha o LLM que melhor se adequa √†s suas necessidades:

- **Privacidade**: Use Ollama para processamento 100% local
- **Qualidade**: Use OpenAI/Gemini/Claude para respostas de alta qualidade
- **Custo**: Use Ollama (gratuito) ou Gemini Free Tier
- **Flexibilidade**: Misture provedores (ex: Ollama para LLM, OpenAI para embeddings)

## üèóÔ∏è Arquitetura

### Factory Pattern

O projeto usa o **Factory Pattern** para abstrair a cria√ß√£o de LLMs:

```python
from agents.llm import LLMFactory, EmbeddingsFactory

# Cria LLM baseado em vari√°vel de ambiente LLM_PROVIDER
llm = LLMFactory.create_llm()

# Cria embeddings baseado em EMBEDDING_PROVIDER
embeddings = EmbeddingsFactory.create_embeddings()
```

### Componentes Principais

```text
agents/llm/
‚îú‚îÄ‚îÄ __init__.py              # LLMProvider, EmbeddingProvider, LLMConfig
‚îú‚îÄ‚îÄ factory.py               # LLMFactory (cria chat models)
‚îî‚îÄ‚îÄ embeddings_factory.py    # EmbeddingsFactory (cria embeddings)
```

### Fluxo de Configura√ß√£o

1. **Leitura de vari√°veis de ambiente** (`.env`)
2. **Valida√ß√£o de configura√ß√£o** (chaves API, modelos)
3. **Instancia√ß√£o do provedor** (via factory)
4. **Uso transparente** (mesmo c√≥digo para todos os provedores)

## üîå Provedores Suportados

### Ollama (Local)

**‚úÖ Vantagens:**

- 100% gratuito
- 100% privado (dados n√£o saem da sua m√°quina)
- Sem necessidade de API keys
- Funciona offline
- V√°rios modelos dispon√≠veis

**‚ùå Desvantagens:**

- Requer recursos computacionais (RAM, GPU opcional)
- Velocidade depende do hardware
- Qualidade pode ser inferior a modelos comerciais

**üì¶ Instala√ß√£o:**

```bash
# J√° inclu√≠do nas depend√™ncias padr√£o
uv sync

# Iniciar Ollama via Docker
docker-compose up -d ollama

# Baixar modelos
docker exec -it ollama ollama pull qwen2.5:3b
docker exec -it ollama ollama pull nomic-embed-text
```

**‚öôÔ∏è Configura√ß√£o (.env):**

```bash
LLM_PROVIDER=ollama
EMBEDDING_PROVIDER=ollama

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

**üéØ Modelos Recomendados:**

| Modelo | Tamanho | RAM Necess√°ria | Uso |
|--------|---------|----------------|-----|
| `qwen2.5:3b` | 3B | 4GB | Padr√£o, r√°pido |
| `qwen2.5:7b` | 7B | 8GB | Melhor qualidade |
| `llama3.2:3b` | 3B | 4GB | Alternativa |
| `mistral:7b` | 7B | 8GB | Boa qualidade |
| `nomic-embed-text` | - | 1GB | Embeddings |

### OpenAI

**‚úÖ Vantagens:**

- Alta qualidade de respostas
- Velocidade r√°pida
- API confi√°vel e est√°vel
- Bons embeddings

**‚ùå Desvantagens:**

- Pago (custo por token)
- Dados enviados para servidores da OpenAI
- Requer API key

**üì¶ Instala√ß√£o:**

```bash
# Instalar depend√™ncia opcional
uv sync --extra openai
```

**üîë Obter API Key:**

1. Acesse: <https://platform.openai.com/api-keys>
2. Crie uma nova chave (ou use existente)
3. Copie e guarde em local seguro

**‚öôÔ∏è Configura√ß√£o (.env):**

```bash
LLM_PROVIDER=openai
EMBEDDING_PROVIDER=openai

OPENAI_API_KEY=sk-proj-...  # Sua chave aqui
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Opcional: custom base URL (para APIs compat√≠veis)
# OPENAI_BASE_URL=https://api.openai.com/v1
```

**üéØ Modelos Recomendados:**

| Modelo | Custo (1M tokens) | Velocidade | Uso |
|--------|-------------------|------------|-----|
| `gpt-4o-mini` | $0.15 / $0.60 | R√°pido | ‚≠ê Padr√£o |
| `gpt-4o` | $2.50 / $10.00 | M√©dio | Alta qualidade |
| `gpt-3.5-turbo` | $0.50 / $1.50 | Muito r√°pido | Econ√¥mico |
| `text-embedding-3-small` | $0.02 | R√°pido | Embeddings |
| `text-embedding-3-large` | $0.13 | R√°pido | Embeddings HD |

**üí∞ Estimativa de Custo:**

Para um chat t√≠pico de reparo residencial:

- Mensagem m√©dia: ~500 tokens (input + output)
- 100 conversas: ~50,000 tokens
- Custo estimado com GPT-4o-mini: **~$0.04**

### Google Gemini

**‚úÖ Vantagens:**

- Free tier generoso (15 requisi√ß√µes/minuto)
- Alta qualidade de respostas
- Velocidade r√°pida
- √ìtimos embeddings

**‚ùå Desvantagens:**

- Dados enviados para servidores do Google
- Limite de requisi√ß√µes no free tier
- API menos madura que OpenAI

**üì¶ Instala√ß√£o:**

```bash
# Instalar depend√™ncia opcional
uv sync --extra google
```

**üîë Obter API Key:**

1. Acesse: <https://makersuite.google.com/app/apikey>
2. Crie uma nova chave (ou use existente)
3. Copie e guarde em local seguro

**‚öôÔ∏è Configura√ß√£o (.env):**

```bash
LLM_PROVIDER=gemini
EMBEDDING_PROVIDER=gemini

GEMINI_API_KEY=AIza...  # Sua chave aqui
GEMINI_MODEL=gemini-1.5-flash
GEMINI_EMBEDDING_MODEL=models/embedding-001
```

**üéØ Modelos Recomendados:**

| Modelo | Custo | Free Tier | Uso |
|--------|-------|-----------|-----|
| `gemini-1.5-flash` | $0.35/$1.05 | ‚úÖ 15 req/min | ‚≠ê Padr√£o |
| `gemini-1.5-pro` | $3.50/$10.50 | ‚úÖ 2 req/min | Alta qualidade |
| `models/embedding-001` | Gratuito | ‚úÖ Sim | Embeddings |

### Anthropic (Claude)

**‚úÖ Vantagens:**

- Alt√≠ssima qualidade de respostas
- Excelente para racioc√≠nio complexo
- Contexto longo (200k tokens)
- Seguro e confi√°vel

**‚ùå Desvantagens:**

- Mais caro que OpenAI
- N√£o possui embeddings pr√≥prios
- Requer API key paga

**üì¶ Instala√ß√£o:**

```bash
# Instalar depend√™ncia opcional
uv sync --extra anthropic

# Como Anthropic n√£o tem embeddings, instale tamb√©m outro provedor
uv sync --extra anthropic --extra openai
```

**üîë Obter API Key:**

1. Acesse: <https://console.anthropic.com/settings/keys>
2. Crie uma nova chave
3. Copie e guarde em local seguro

**‚öôÔ∏è Configura√ß√£o (.env):**

```bash
LLM_PROVIDER=anthropic
EMBEDDING_PROVIDER=openai  # Ou ollama/gemini

ANTHROPIC_API_KEY=sk-ant-...  # Sua chave aqui
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Configure tamb√©m o provedor de embeddings
OPENAI_API_KEY=sk-proj-...
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

**üéØ Modelos Recomendados:**

| Modelo | Custo (1M tokens) | Contexto | Uso |
|--------|-------------------|----------|-----|
| `claude-3-5-sonnet-20241022` | $3.00 / $15.00 | 200k | ‚≠ê Padr√£o |
| `claude-3-5-haiku-20241022` | $0.80 / $4.00 | 200k | Econ√¥mico |
| `claude-3-opus-20240229` | $15.00 / $75.00 | 200k | M√°xima qualidade |

## ‚öôÔ∏è Configura√ß√£o

### Arquivo .env Completo

```bash
# ============================================================
# PROVEDOR PRINCIPAL
# ============================================================
LLM_PROVIDER=ollama  # ollama, openai, gemini, anthropic

# Provedor de embeddings (pode ser diferente do LLM)
EMBEDDING_PROVIDER=ollama

# Configura√ß√µes gerais
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=500

# ============================================================
# OLLAMA
# ============================================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:3b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# ============================================================
# OPENAI
# ============================================================
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# ============================================================
# GOOGLE GEMINI
# ============================================================
GEMINI_API_KEY=
GEMINI_MODEL=gemini-1.5-flash
GEMINI_EMBEDDING_MODEL=models/embedding-001

# ============================================================
# ANTHROPIC
# ============================================================
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

### Valida√ß√£o de Configura√ß√£o

O sistema valida automaticamente:

```python
from agents.llm import LLMConfig

# Valida provedor atual
LLMConfig.validate_config()

# Valida provedor espec√≠fico
LLMConfig.validate_config(provider=LLMProvider.OPENAI)
```

## üîÑ Migra√ß√£o entre Provedores

### Cen√°rio 1: Desenvolvimento Local ‚Üí Produ√ß√£o Cloud

**Desenvolvimento (Ollama):**

```bash
LLM_PROVIDER=ollama
OLLAMA_MODEL=qwen2.5:3b
```

**Produ√ß√£o (OpenAI):**

```bash
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o-mini
```

### Cen√°rio 2: Provedor H√≠brido

Use LLM comercial + embeddings locais:

```bash
LLM_PROVIDER=openai
EMBEDDING_PROVIDER=ollama

OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o-mini

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
```

### Checklist de Migra√ß√£o

Caso voc√™ mude de provedor, siga este checklist:

- [ ] Instalar depend√™ncias do novo provedor (`uv sync --extra <provider>`)
- [ ] Obter API key (se necess√°rio)
- [ ] Atualizar `.env` com novas configura√ß√µes
- [ ] Testar com `uv run python api/app.py` ou CLI
- [ ] Recriar RAG se mudar provedor de embeddings (`uv run scripts/setup_rag.py`)
- [ ] Validar custos (se API paga)

## üêõ Troubleshooting

### Erro: "OPENAI_API_KEY n√£o configurada"

**Solu√ß√£o:** Configure a vari√°vel no `.env`:

```bash
OPENAI_API_KEY=sk-proj-sua-chave-aqui
```

### Erro: "Import langchain_openai could not be resolved"

**Solu√ß√£o:** Instale a depend√™ncia opcional:

```bash
uv sync --extra openai
```

### Erro: "Ollama connection refused"

**Solu√ß√£o:** Certifique-se que o Ollama est√° rodando:

```bash
docker-compose up -d ollama
docker ps | grep ollama
```

### Erro: "Rate limit exceeded" (Gemini)

**Solu√ß√£o:** Voc√™ ultrapassou o free tier (15 req/min). Espere 1 minuto ou:

```bash
# Adicione delays entre requisi√ß√µes
LLM_TEMPERATURE=0.3  # Menor = mais consistente
```

### Erro: "Vector store incompat√≠vel" ap√≥s mudar embedding provider

**Solu√ß√£o:** Recrie o banco vetorial:

```bash
# Backup (opcional)
mv chroma_db chroma_db.bak

# Recriar com novo provedor
uv run scripts/setup_rag.py
```

### Performance Lenta com Ollama

**Solu√ß√µes:**

1. Use modelo menor: `qwen2.5:3b` ao inv√©s de `7b`
2. Se tiver GPU NVIDIA, configure CUDA:

   ```bash
   docker-compose down
   # Edite docker-compose.yml para usar GPU
   docker-compose up -d
   ```

3. Reduza `num_predict`:

   ```bash
   LLM_MAX_TOKENS=300  # Ao inv√©s de 500
   ```

## üìö Recursos Adicionais

- [LangChain Docs](https://python.langchain.com/docs/integrations/llms/)
- [Ollama Library](https://ollama.com/library)
- [OpenAI Platform](https://platform.openai.com/docs)
- [Google AI Studio](https://ai.google.dev/)
- [Anthropic Docs](https://docs.anthropic.com/)
