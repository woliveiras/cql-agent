# ðŸ”Œ MÃ³dulo LLM - Gerenciamento de Provedores

Este mÃ³dulo implementa uma camada de abstraÃ§Ã£o para gerenciar diferentes provedores de LLM (Large Language Models) e embeddings.

## ðŸ“‹ VisÃ£o Geral

O mÃ³dulo `agents/llm` permite que o CQL Agent suporte mÃºltiplos provedores de forma transparente, usando o **Factory Pattern**.

### Provedores Suportados

- **Ollama** - Modelos locais (padrÃ£o)
- **OpenAI** - GPT-4, GPT-3.5-turbo, etc
- **Google Gemini** - Gemini 1.5 Flash/Pro
- **Anthropic** - Claude 3.5 Sonnet

## ðŸ—ï¸ Arquitetura

### Componentes

```text
agents/llm/
â”œâ”€â”€ __init__.py              # Exports principais + enums + configuraÃ§Ã£o
â”œâ”€â”€ factory.py               # LLMFactory - cria chat models
â””â”€â”€ embeddings_factory.py    # EmbeddingsFactory - cria embeddings
```

### Factory Pattern

```python
# Uso bÃ¡sico
from agents.llm import LLMFactory, EmbeddingsFactory

# Cria LLM baseado em variÃ¡vel de ambiente
llm = LLMFactory.create_llm()

# Cria embeddings
embeddings = EmbeddingsFactory.create_embeddings()
```

### Fluxo de CriaÃ§Ã£o

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client Code        â”‚
â”‚  (RepairAgent, etc) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLMFactory /       â”‚
â”‚  EmbeddingsFactory  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€ LÃª LLM_PROVIDER / EMBEDDING_PROVIDER
           â”œâ”€ Valida configuraÃ§Ã£o
           â””â”€ Instancia provedor correto
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chat Model /       â”‚
â”‚  Embeddings         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“¦ Componentes Detalhados

### 1. `__init__.py`

**Exports:**

- `LLMProvider` - Enum com provedores de LLM
- `EmbeddingProvider` - Enum com provedores de embeddings
- `LLMConfig` - Classe de configuraÃ§Ã£o estÃ¡tica
- `LLMFactory` - Factory para criar LLMs
- `EmbeddingsFactory` - Factory para criar embeddings

**Exemplo:**

```python
from agents.llm import LLMProvider, LLMConfig

# Obter provedor configurado
provider = LLMConfig.get_provider()

# Validar configuraÃ§Ã£o
LLMConfig.validate_config()

# Acessar configuraÃ§Ãµes
print(LLMConfig.OLLAMA_MODEL)
print(LLMConfig.OPENAI_API_KEY)
```

### 2. `factory.py` - LLMFactory

**Responsabilidades:**

- Criar instÃ¢ncias de chat models
- Validar configuraÃ§Ã£o antes de criar
- Fornecer mensagens de erro Ãºteis

**MÃ©todos:**

```python
class LLMFactory:
    @staticmethod
    def create_llm(
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> BaseChatModel:
        """Cria LLM do provedor especificado"""
```

**Exemplo:**

```python
from agents.llm import LLMFactory, LLMProvider

# Usar provedor do .env
llm = LLMFactory.create_llm()

# ForÃ§ar provedor especÃ­fico
llm = LLMFactory.create_llm(
    provider=LLMProvider.OPENAI,
    model="gpt-4o-mini",
    temperature=0.7
)
```

### 3. `embeddings_factory.py` - EmbeddingsFactory

**Responsabilidades:**

- Criar instÃ¢ncias de embeddings
- Suportar provedores diferentes do LLM principal
- Validar API keys

**MÃ©todos:**

```python
class EmbeddingsFactory:
    @staticmethod
    def create_embeddings(
        provider: Optional[EmbeddingProvider] = None,
        model: Optional[str] = None,
        **kwargs
    ) -> Embeddings:
        """Cria embeddings do provedor especificado"""
```

**Exemplo:**

```python
from agents.llm import EmbeddingsFactory, EmbeddingProvider

# Usar provedor do .env
embeddings = EmbeddingsFactory.create_embeddings()

# ForÃ§ar provedor especÃ­fico
embeddings = EmbeddingsFactory.create_embeddings(
    provider=EmbeddingProvider.OPENAI,
    model="text-embedding-3-small"
)
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Todas as configuraÃ§Ãµes sÃ£o lidas do arquivo `.env`:

```bash
# Provedor principal
LLM_PROVIDER=ollama  # ollama, openai, gemini, anthropic

# Provedor de embeddings (opcional, usa LLM_PROVIDER se nÃ£o especificado)
EMBEDDING_PROVIDER=ollama

# ConfiguraÃ§Ãµes gerais
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=500

# ConfiguraÃ§Ãµes especÃ­ficas de cada provedor
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:3b

OPENAI_API_KEY=sk-proj-...
OPENAI_MODEL=gpt-4o-mini

GEMINI_API_KEY=AIza...
GEMINI_MODEL=gemini-1.5-flash

ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
```

### Classe LLMConfig

```python
class LLMConfig:
    """ConfiguraÃ§Ã£o centralizada para LLM providers"""
    
    @staticmethod
    def get_provider() -> LLMProvider:
        """Retorna provedor de LLM configurado"""
    
    @staticmethod
    def get_embedding_provider() -> EmbeddingProvider:
        """Retorna provedor de embeddings configurado"""
    
    @classmethod
    def validate_config(cls, provider: Optional[LLMProvider] = None) -> None:
        """Valida se configuraÃ§Ã£o estÃ¡ completa"""
        
    # Atributos de classe com todas as configuraÃ§Ãµes
    OLLAMA_BASE_URL: str
    OLLAMA_MODEL: str
    OPENAI_API_KEY: str
    OPENAI_MODEL: str
    # ... etc
```

## ðŸ”„ IntegraÃ§Ã£o com CÃ³digo Existente

```python
from agents.llm import LLMFactory, EmbeddingsFactory

# Cria automaticamente com base em .env
llm = LLMFactory.create_llm()
embeddings = EmbeddingsFactory.create_embeddings()
```

## ðŸ§ª Testes

### Testar Provedor

```bash
# Executar script de demonstraÃ§Ã£o
uv run scripts/demo_llm_providers.py
```

### Testar Manualmente

```python
from agents.llm import LLMFactory, LLMConfig

# Validar configuraÃ§Ã£o
try:
    LLMConfig.validate_config()
    print("âœ… ConfiguraÃ§Ã£o vÃ¡lida")
except ValueError as e:
    print(f"âŒ Erro: {e}")

# Criar e testar LLM
llm = LLMFactory.create_llm()
response = llm.invoke("OlÃ¡!")
print(response.content)
```

## ðŸš€ Adicionar Novo Provedor

Para adicionar suporte a um novo provedor (ex: Cohere, HuggingFace):

1. **Adicionar enum em `__init__.py`:**

   ```python
   class LLMProvider(str, Enum):
       # ... existentes ...
       COHERE = "cohere"
   ```

2. **Adicionar configuraÃ§Ãµes em `LLMConfig`:**

   ```python
   COHERE_API_KEY = os.getenv("COHERE_API_KEY")
   COHERE_MODEL = os.getenv("COHERE_MODEL", "command")
   ```

3. **Adicionar mÃ©todo em `factory.py`:**

   ```python
   @staticmethod
   def _create_cohere_llm(...) -> BaseChatModel:
       from langchain_cohere import ChatCohere
       return ChatCohere(...)
   ```

4. **Adicionar caso no `create_llm()`:**

   ```python
   elif provider == LLMProvider.COHERE:
       return LLMFactory._create_cohere_llm(...)
   ```

5. **Adicionar dependÃªncia em `pyproject.toml`:**

   ```toml
   [project.optional-dependencies]
   cohere = ["langchain-cohere>=0.1.0"]
   ```

## ðŸ“š ReferÃªncias

- [LangChain - Model I/O](https://python.langchain.com/docs/modules/model_io/)
- [Factory Pattern](https://refactoring.guru/design-patterns/factory-method)
- [Guia Completo de Provedores](../../docs/LLM_PROVIDERS.md)
